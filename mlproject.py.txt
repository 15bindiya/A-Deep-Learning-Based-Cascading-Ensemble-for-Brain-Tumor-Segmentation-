# -*- coding: utf-8 -*-
"""MLproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cmqTEgw350BiRv5lcbLZGofSz1gM4hcm
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install keras
!pip install --upgrade tensorflow

#https://www.youtube.com/watch?v=M3EZS__Z_XE
#https://github.com/nikhilroxtomar/UNet-Segmentation-in-Keras-TensorFlow/blob/master/unet-segmentation.ipynb
## Imports
import os
import sys
import random

import numpy as np
import cv2
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras

## Seeding 
seed = 2019
random.seed = seed
np.random.seed = seed
tf.seed = seed

class DataGen(keras.utils.Sequence):
    def __init__(self, ids, path, batch_size=8, image_size=128):
        self.ids = ids
        self.path = path
        self.batch_size = batch_size
        self.image_size = image_size
        self.on_epoch_end()
        
    def __load__(self, id_name):
        ## Path
        image_path = os.path.join(self.path, "Data", id_name) + ".tif"
        mask_path = os.path.join(self.path, "Mask", id_name) + "_mask.tif"
        ## Reading Image
        image = cv2.imread(image_path, 1)
        image = cv2.resize(image, (self.image_size, self.image_size))
        _mask_image = cv2.imread(mask_path, -1)
        
        mask = cv2.resize(_mask_image, (self.image_size, self.image_size)) #128x128
        ## Normalizaing 
        image = image/255.0
        mask = mask/255.0
        
        return image, mask
    
    def __getitem__(self, index):
        if(index+1)*self.batch_size > len(self.ids):
            self.batch_size = len(self.ids) - index*self.batch_size
        
        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]
        
        image = []
        mask  = []
        
        for id_name in files_batch:
            _img, _mask = self.__load__(id_name)
            image.append(_img)
            mask.append(_mask)
            
        image = np.array(image)
        mask  = np.array(mask)
        
        return image, mask
    
    def on_epoch_end(self):
        pass
    
    def __len__(self):
        return int(np.ceil(len(self.ids)/float(self.batch_size)))

import datetime
target_data = []
loss = []
class History_LAW(keras.callbacks.Callback):
    i = 0
    def on_epoch_end(self, epoch, logs={}):
        self.i += 1
   # def on_train_batch_begin(self, batch, logs=None):
   #     if i == 4:
     #   print('{} Training: batch {} begins at {}'.format(self.i, batch, datetime.datetime.now().time()))

    def on_train_batch_end(self, batch, logs=None):
        
        if self.i == 99:
          if logs['loss'] > .10:
            target_data.append(batch)

    #def on_test_epoch_begin(self, epoch, logs=None):
        #print('Evaluating: epoch {} begins at {}'.format(epoch, datetime.datetime.now().time()))

    #def on_test_batch_begin(self, batch, logs=None):
        #print('Evaluating: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))

    #def on_test_batch_end(self, batch, logs=None):
        #print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))
        #print('For batch {}, loss is {:7.2f}.'.format(batch, logs['loss']))

image_size = 512
train_path = "/content/drive/My Drive/Dataset_Brain_Tumour (copy)/"
epochs = 100
batch_size = 1

## Training Ids
train_ids = next(os.walk(train_path))[1]
image_ids=[]
for r, d, f in os.walk(train_path):
    for file in f:
        if "mask.tif" in file:
            fil=(file.split("_mask.tif"))[0]
            image_ids.append(fil)

count = len(image_ids)
test_size = int(count * (15 / 100))
validation_size = int(count * (15 / 100))
train_size = int(count * (70 / 100))
test_data = image_ids[:test_size]
train = image_ids[test_size:]

valid_data = train[:validation_size]
train_data = train[validation_size:]

gen = DataGen(train_data, train_path, batch_size=batch_size, image_size=image_size)

x, y = gen.__getitem__(0)
print(x.shape, y.shape)

r = random.randint(0, len(x)-1)

fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
ax = fig.add_subplot(1, 2, 1)
ax.imshow(x[r])
ax = fig.add_subplot(1, 2, 2)
ax.imshow(np.reshape(y[r], (image_size, image_size)), cmap="gray")
plt.show()

def down_block(x, filters, kernel_size=(3, 3), padding="same", strides=1):
    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(x)
    c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                        beta_initializer='zeros', gamma_initializer='ones',
                                        moving_mean_initializer='zeros',
                                        moving_variance_initializer='ones', beta_regularizer=None,
                                        gamma_regularizer=None,
                                        beta_constraint=None, gamma_constraint=None)(c)
    #c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(c)
    #c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
    #                                    beta_initializer='zeros', gamma_initializer='ones',
    #                                    moving_mean_initializer='zeros',
    #                                    moving_variance_initializer='ones', beta_regularizer=None,
    #                                    gamma_regularizer=None,
    #                                    beta_constraint=None, gamma_constraint=None)(c)
    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)
    return c, p

def up_block(x, skip, filters, kernel_size=(3, 3), padding="same", strides=1):
    us = keras.layers.UpSampling2D((2, 2))(x)
    concat = keras.layers.Concatenate()([us, skip])
    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(concat)
    c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                        beta_initializer='zeros', gamma_initializer='ones',
                                        moving_mean_initializer='zeros',
                                        moving_variance_initializer='ones', beta_regularizer=None,
                                        gamma_regularizer=None,
                                        beta_constraint=None, gamma_constraint=None)(c)
    #c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(c)
    #c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
    #                                    beta_initializer='zeros', gamma_initializer='ones',
    #                                    moving_mean_initializer='zeros',
    #                                    moving_variance_initializer='ones', beta_regularizer=None,
    #                                    gamma_regularizer=None,
    #                                    beta_constraint=None, gamma_constraint=None)(c)
    return c

def bottleneck(x, filters, kernel_size=(3, 3), padding="same", strides=1):
    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(x)
    c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                        beta_initializer='zeros', gamma_initializer='ones',
                                        moving_mean_initializer='zeros',
                                        moving_variance_initializer='ones', beta_regularizer=None,
                                        gamma_regularizer=None,
                                        beta_constraint=None, gamma_constraint=None)(c)
    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(c)
    c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
                                        beta_initializer='zeros', gamma_initializer='ones',
                                        moving_mean_initializer='zeros',
                                        moving_variance_initializer='ones', beta_regularizer=None,
                                        gamma_regularizer=None,
                                        beta_constraint=None, gamma_constraint=None)(c)
    #c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation="relu")(c)
    #c = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,
    #                                    beta_initializer='zeros', gamma_initializer='ones',
    #                                    moving_mean_initializer='zeros',
    #                                    moving_variance_initializer='ones', beta_regularizer=None,
    #                                    gamma_regularizer=None,
    #                                    beta_constraint=None, gamma_constraint=None)(c)
    return c

def UNet():
    f = [8, 16, 32, 64, 128, 256, 512, 1024]
    inputs = keras.layers.Input((image_size, image_size, 3))
    
    p0 = inputs
    c1, p1 = down_block(p0, f[0]) #128 -> 64
    #c2, p2 = down_block(p1, f[1], (5,5)) #64 -> 32
    #c3, p3 = down_block(p2, f[2]) #32 -> 16
    #c4, p4 = down_block(p3, f[3]) #16->8
    
    bn = bottleneck(p1, f[3])
    
    u1 = up_block(bn, c1, f[0]) #8 -> 16
    #u2 = up_block(u1, c1, f[0]) #16 -> 32
    #u3 = up_block(u2, c2, f[1]) #32 -> 64
    #u4 = up_block(u3, c1, f[0]) #64 -> 128
    #c = keras.layers.Conv2D(64, (3,3), padding="same", strides=1, activation="relu")(u4)
    outputs = keras.layers.Conv2D(1, (1, 1), padding="same", activation="sigmoid")(u1)
    model = keras.models.Model(inputs, outputs)
    return model

model = UNet()
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["acc"])
model.summary()

train_gen = DataGen(train_data, train_path, image_size=image_size, batch_size=batch_size)
valid_gen = DataGen(valid_data, train_path, image_size=image_size, batch_size=batch_size)

train_steps = len(train_data)//batch_size
valid_steps = len(valid_data)//batch_size

model_Hist = History_LAW()
seqModel=model.fit_generator(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps, validation_steps=valid_steps, 
                    epochs=epochs, callbacks = [model_Hist])
train_loss1 = seqModel.history['loss']
val_loss1   = seqModel.history['val_loss']
train_acc1  = seqModel.history['acc']
val_acc1    = seqModel.history['val_acc']

plt.plot(train_loss1)
plt.plot(val_loss1 )
plt.title('Model 1 loss')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(train_acc1)
plt.plot(val_acc1 )
plt.title('Model 1 Accuracy')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

new_train_data = []
for i in target_data:
  new_train_data.append(train_data[i])
print(new_train_data)

## Save the Weights
model.save_weights("UNetW.h5")


j=0
for id in valid_data:
    ## Dataset for prediction
    x, y = valid_gen.__getitem__(j)
    result = model.predict(x)
    j += 2
    result = result > 0.5
    i=0
    while i < len(x):
        fig = plt.figure(6,10)
        fig.subplots_adjust(hspace=0.4, wspace=0.4)
        ax = fig.add_subplot(1, 3, 1)
        ax.imshow(np.reshape(y[i]*255, (image_size, image_size)), cmap="gray")

        ax = fig.add_subplot(1, 3, 2)
        ax.imshow(np.reshape(result[i]*255, (image_size, image_size)), cmap="gray")

        ax = fig.add_subplot(1, 3, 3)
        ax.imshow(x[i])

        plt.show()

        i += 1

def UNet2():
    f = [8, 16, 32, 64, 128, 256, 512, 1024]
    inputs = keras.layers.Input((image_size, image_size, 3))
    
    p0 = inputs
    c1, p1 = down_block(p0, f[0], (5,5)) #128 -> 64
    #c2, p2 = down_block(p1, f[1], (5,5)) #64 -> 32
    #c3, p3 = down_block(p2, f[2]) #32 -> 16
    #c4, p4 = down_block(p3, f[3]) #16->8
    
    bn = bottleneck(p1, f[3])
    
    u1 = up_block(bn, c1, f[0], (5,5)) #8 -> 16
    #u2 = up_block(u1, c1, f[0]) #16 -> 32
    #u3 = up_block(u2, c2, f[1]) #32 -> 64
    #u4 = up_block(u3, c1, f[0]) #64 -> 128
    #c = keras.layers.Conv2D(64, (3,3), padding="same", strides=1, activation="relu")(u4)
    outputs = keras.layers.Conv2D(1, (1, 1), padding="same", activation="sigmoid")(u1)
    model = keras.models.Model(inputs, outputs)
    return model



model2 = UNet2()
model2.compile(optimizer="adam", loss="binary_crossentropy", metrics=["acc"])
model2.summary()

train_gen = DataGen(new_train_data, train_path, image_size=image_size, batch_size=batch_size)
valid_gen = DataGen(valid_data, train_path, image_size=image_size, batch_size=batch_size)

train_steps = len(new_train_data)//batch_size
valid_steps = len(valid_data)//batch_size

model_Hist = History_LAW()
target_data.clear()
seqModel = model2.fit_generator(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps, validation_steps=valid_steps, 
                    epochs=epochs, callbacks = [model_Hist])
train_loss2 = seqModel.history['loss']
val_loss2   = seqModel.history['val_loss']
train_acc2  = seqModel.history['acc']
val_acc2    = seqModel.history['val_acc']

plt.plot(train_loss2)
plt.plot(val_loss2 )
plt.title('Model 2 loss')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(train_acc2)
plt.plot(val_acc2 )
plt.title('Model 2 Accuracy')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

new_train_data = []
for i in target_data:
  new_train_data.append(train_data[i])

print(new_train_data)

def UNet3():
    f = [8, 16, 32, 64, 128, 256, 512, 1024]
    inputs = keras.layers.Input((image_size, image_size, 3))
    
    p0 = inputs
    c1, p1 = down_block(p0, f[0] )#128 -> 64
    c2, p2 = down_block(p1, f[1],(5,5)) #64 -> 32
    #c3, p3 = down_block(p2, f[2]) #32 -> 16
    #c4, p4 = down_block(p3, f[3]) #16->8
    
    bn = bottleneck(p2, f[3], (7,7))
    
    u1 = up_block(bn, c2, f[1],(5,5)) #8 -> 16
    u2 = up_block(u1, c1, f[0]) #16 -> 32
    #u3 = up_block(u2, c2, f[1]) #32 -> 64
    #u4 = up_block(u3, c1, f[0]) #64 -> 128
    #c = keras.layers.Conv2D(64, (3,3), padding="same", strides=1, activation="relu")(u4)
    outputs = keras.layers.Conv2D(1, (1, 1), padding="same", activation="sigmoid")(u2)
    model = keras.models.Model(inputs, outputs)
    return model

model3 = UNet3()
model3.compile(optimizer="adam", loss="binary_crossentropy", metrics=["acc"])
model3.summary()

train_gen = DataGen(new_train_data, train_path, image_size=image_size, batch_size=batch_size)
valid_gen = DataGen(valid_data, train_path, image_size=image_size, batch_size=batch_size)

train_steps = len(new_train_data)//batch_size
valid_steps = len(valid_data)//batch_size

model_Hist = History_LAW()
target_data.clear()
seqModel = model3.fit_generator(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps, validation_steps=valid_steps, 
                    epochs=epochs, callbacks = [model_Hist])
train_loss3 = seqModel.history['loss']
val_loss3   = seqModel.history['val_loss']
train_acc3  = seqModel.history['acc']
val_acc3    = seqModel.history['val_acc']

plt.plot(train_loss3)
plt.plot(val_loss3 )
plt.title('Model 3 loss')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(train_acc3)
plt.plot(val_acc3 )
plt.title('Model 3 Accuracy')
plt.ylabel('Percentage')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

## Save the Weights
model.save_weights("Model2.h5")


j=0
for id in valid_data:
    ## Dataset for prediction
    x, y = valid_gen.__getitem__(j)
    result1 = model.predict(x)
    result1 = result1 > 0.5
    result2 = model2.predict(x)
    result2 = result2 > 0.5

    result3 = model3.predict(x)
    result3 = result3 > 0.5

    j += 2
    if np.count_nonzero(result1)>=np.count_nonzero(result2) and np.count_nonzero(result1>=result3): 
      result = result1
    if np.count_nonzero(result2>=result1) and np.count_nonzero(result2>=result3): 
      result = result2
    if np.count_nonzero(result3)>=np.count_nonzero(result2) and np.count_nonzero(result3)>=np.count_nonzero(result1): 
      result = result3
    i=0
    while i < len(x):
        fig.subplots_adjust(hspace=0.4, wspace=0.4)
        ax = fig.add_subplot(1, 3, 1)
        ax.imshow(np.reshape(y[i]*255, (image_size, image_size)), cmap="gray")
        ax.set_title('Mask')
        ax = fig.add_subplot(1, 3, 2)
        ax.imshow(np.reshape(result[i]*255, (image_size, image_size)), cmap="gray")
        ax.set_title('Result')
        ax = fig.add_subplot(1, 3, 3)
        ax.imshow(x[i])
        ax.set_title('Image')

        plt.show()

        i += 1
